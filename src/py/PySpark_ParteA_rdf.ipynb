{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb89f2f",
   "metadata": {},
   "source": [
    "# Proyecto Parcial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e94c3962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%info` not found.\n"
     ]
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe28cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%configure` not found.\n"
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python\",\n",
    "          \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "          \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "          \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236a8c5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Importar funciones únicas\n",
    "from pyspark.sql.functions import lit, col\n",
    "\n",
    "# Importar funciones con un alias, que pudieran \n",
    "# tener conflicto con las librerías base de Python, \n",
    "# como sum, avg, round, ...\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StringType, TimestampType, \n",
    "    StringType, StringType, FloatType, StructField, \n",
    "    LongType, IntegerType)\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "raw_def_columns = StructType([\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"presentacion\", StringType(), True),\n",
    "    StructField(\"marca\", StringType(), True),\n",
    "    StructField(\"tipo\", StringType(), True),\n",
    "    StructField(\"catalogo\", StringType(), True),\n",
    "    StructField(\"precio\", FloatType(), True),\n",
    "    StructField(\"fecha\", StringType(), True),\n",
    "    StructField(\"tienda\", StringType(), True),\n",
    "    StructField(\"tipo_tienda\", LongType(), True),\n",
    "    StructField(\"sucursal\", StringType(), True),\n",
    "    StructField(\"direccion\", StringType(), True),\n",
    "    StructField(\"estado\", StringType(), True),\n",
    "    StructField(\"ciudad\", StringType(), True),\n",
    "    StructField(\"latitud\", IntegerType(),True),\n",
    "    StructField(\"longitud\", IntegerType(), True)\n",
    "]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7c1ef",
   "metadata": {},
   "source": [
    "Recuerda que vamos a trabajar con spark dataframes, por lo tanto tienes que abrir un SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4f5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/05/03 14:27:17 WARN Utils: Your hostname, ThomasLOQ resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/03 14:27:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/03 14:27:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CleanCompleteFile\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706f8f87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'install_pypi_package'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minstall_pypi_package\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SparkContext' object has no attribute 'install_pypi_package'"
     ]
    }
   ],
   "source": [
    "spark._sc.install_pypi_package(\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d468deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket\n",
    "NAME = 'thmrudolf' ##CAMBIAR AQUÍ SU NOMBRE.\n",
    "BUCKET = f\"s3://itam-analytics-{NAME}\"\n",
    "FOLDER = 'profeco'\n",
    "\n",
    "\n",
    "# type of catalog\n",
    "CATALOG_TYPE = 'basicos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62172bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 14:27:41 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3://itam-analytics-thmrudolf/profeco/raw_descomprimidos/2024/*.csv.\n",
      "org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:366)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o34.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Descarga de datos de S3\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#raw_df = spark.read.csv(f\"{BUCKET}/{FOLDER}/raw_descomprimidos/*/*.csv\", \u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m raw_df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mFOLDER\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/raw_descomprimidos/2024/*.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                        \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw_def_columns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    739\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunc\u001b[39m(iterator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o34.csv.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "#Descarga de datos de S3\n",
    "#raw_df = spark.read.csv(f\"{BUCKET}/{FOLDER}/raw_descomprimidos/*/*.csv\", \n",
    "raw_df = spark.read.csv(f\"{BUCKET}/{FOLDER}/raw_descomprimidos/2024/*.csv\", \n",
    "                        header=True, \n",
    "                        inferSchema=True, \n",
    "                       schema=raw_def_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93d3bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mraw_df\u001b[49m.printSchema()\n",
      "\u001b[31mNameError\u001b[39m: name 'raw_df' is not defined"
     ]
    }
   ],
   "source": [
    "raw_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dc207",
   "metadata": {},
   "source": [
    "### Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e0a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columnas = [\n",
    "    \"producto\", \"presentacion\", \"marca\", \"tipo\", \"catalogo\",\n",
    "    \"precio\", \"fecha\", \"tienda\", \"tipo_tienda\", \"sucursal\",\n",
    "    \"direccion\", \"estado\", \"ciudad\", \"latitud\", \"longitud\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfeff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for column in raw_df.columns:\n",
    "    raw_df = raw_df.withColumn(column, regexp_replace(col(column), '\"\"', \"'\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db5673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+-----------+----------+--------------+------------+-----------+------------+-----------------+--------------+---------------+------------+------------+-------------+--------------+\n",
      "|producto_count|presentacion_count|marca_count|tipo_count|catalogo_count|precio_count|fecha_count|tienda_count|tipo_tienda_count|sucursal_count|direccion_count|estado_count|ciudad_count|latitud_count|longitud_count|\n",
      "+--------------+------------------+-----------+----------+--------------+------------+-----------+------------+-----------------+--------------+---------------+------------+------------+-------------+--------------+\n",
      "|             0|                 0|          0|         0|             0|           0|          0|           0|             NULL|             0|              0|           0|           0|         NULL|          NULL|\n",
      "+--------------+------------------+-----------+----------+--------------+------------+-----------+------------+-----------------+--------------+---------------+------------+------------+-------------+--------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Count occurrences of `\"\"` before replacement\n",
    "counts = [expr(f\"sum(length({c}) - length(regexp_replace({c}, '\\\"\\\"', '')))\").alias(f\"{c}_count\") for c in raw_df.columns]\n",
    "\n",
    "# Show count for each column\n",
    "raw_df.select(*counts).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc677e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('producto', 'string'), ('presentacion', 'string'), ('marca', 'string'), ('tipo', 'string'), ('catalogo', 'string'), ('precio', 'string'), ('fecha', 'date'), ('tienda', 'string'), ('tipo_tienda', 'string'), ('sucursal', 'string'), ('direccion', 'string'), ('estado', 'string'), ('ciudad', 'string'), ('latitud', 'string'), ('longitud', 'string'), ('anio', 'int'), ('mes', 'int'), ('dia', 'int')]"
     ]
    }
   ],
   "source": [
    "#Cambiamos tipo a variable fecha\n",
    "# Convertimos columnas a tipo correcto\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "\n",
    "raw_df = raw_df.withColumn(\"fecha\", to_date(col(\"fecha\"), \"yyyy-MM-dd\"))\n",
    "raw_df = raw_df \\\n",
    "    .withColumn(\"anio\", year(col(\"fecha\"))) \\\n",
    "    .withColumn(\"mes\", month(col(\"fecha\"))) \\\n",
    "    .withColumn(\"dia\", dayofmonth(col(\"fecha\")))\n",
    "\n",
    "# Revisamos que esté todo en orden\n",
    "print(raw_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993ff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- producto: string (nullable = true)\n",
      " |-- marca: string (nullable = true)\n",
      " |-- tipo: string (nullable = true)\n",
      " |-- catalogo: string (nullable = true)\n",
      " |-- precio: string (nullable = true)\n",
      " |-- fecha: date (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- anio: integer (nullable = true)\n",
      " |-- mes: integer (nullable = true)\n",
      " |-- dia: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "#Eliminamos las columnas dirección, latitud y longitud\n",
    "col_to_drop =['direccion', 'latitud', 'longitud', 'presentacion', 'tienda', 'tipo_tienda','sucursal']\n",
    "raw_df = raw_df.drop(*col_to_drop)\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2b37a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------------+------+----------+--------------+--------------+----+---+---+\n",
      "|            producto|               marca|                tipo|         catalogo|precio|     fecha|        estado|        ciudad|anio|mes|dia|\n",
      "+--------------------+--------------------+--------------------+-----------------+------+----------+--------------+--------------+----+---+---+\n",
      "|     barra de sonido|                  lg|aparatos electron...|electrodomesticos|4299.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|     barra de sonido|                sony|aparatos electron...|electrodomesticos|4499.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|     barra de sonido|                sony|aparatos electron...|electrodomesticos|4899.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           batidoras|               oster| aparatos electricos|electrodomesticos| 669.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           batidoras|               oster| aparatos electricos|electrodomesticos|2699.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|  bocinas portátiles|                  lg|aparatos electron...|electrodomesticos|2299.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|  bocinas portátiles|             samsung|aparatos electron...|electrodomesticos|5999.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|  bocinas portátiles|                sony|aparatos electron...|electrodomesticos|4399.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           cafeteras|        black+decker| aparatos electricos|electrodomesticos| 769.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           cafeteras|               oster| aparatos electricos|electrodomesticos| 459.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           cafeteras|               oster| aparatos electricos|electrodomesticos|1299.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           cafeteras|t-fal. heliora gr...| aparatos electricos|electrodomesticos| 749.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           celulares|            motorola|aparatos electron...|electrodomesticos|1999.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           celulares|             samsung|aparatos electron...|electrodomesticos|4299.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           celulares|             samsung|aparatos electron...|electrodomesticos|5799.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|           celulares|                 zte|aparatos electron...|electrodomesticos|1449.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|       dvd / blu ray|                sony|aparatos electron...|electrodomesticos|3199.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|electronicos de v...|           microsoft|arts. de esparcim...|electrodomesticos|7899.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|electronicos de v...|           microsoft|arts. de esparcim...|         juguetes|7899.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|electronicos de v...|            nintendo|arts. de esparcim...|electrodomesticos|8749.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "+--------------------+--------------------+--------------------+-----------------+------+----------+--------------+--------------+----+---+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "#Convertimos el contenido a minúsculas for col in raw_df.columns: \n",
    "\n",
    "from pyspark.sql.functions import lower, col\n",
    "\n",
    "# Convert all string columns to lowercase\n",
    "for column in raw_df.columns:\n",
    "    raw_df = raw_df.withColumn(column, lower(col(column)))\n",
    "\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d239a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: Unidecode\n",
      "Successfully installed Unidecode-1.4.0\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag."
     ]
    }
   ],
   "source": [
    "spark._sc.install_pypi_package(\"Unidecode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34517f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+--------------------+-----------------+------+----------+--------------+--------------+----+---+---+\n",
      "|       producto|marca|                tipo|         catalogo|precio|     fecha|        estado|        ciudad|anio|mes|dia|\n",
      "+---------------+-----+--------------------+-----------------+------+----------+--------------+--------------+----+---+---+\n",
      "|barra de sonido|   lg|aparatos electron...|electrodomesticos|4299.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|barra de sonido| sony|aparatos electron...|electrodomesticos|4499.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|barra de sonido| sony|aparatos electron...|electrodomesticos|4899.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|      batidoras|oster| aparatos electricos|electrodomesticos| 669.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "|      batidoras|oster| aparatos electricos|electrodomesticos|2699.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2|\n",
      "+---------------+-----+--------------------+-----------------+------+----------+--------------+--------------+----+---+---+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import unidecode\n",
    "\n",
    "# List of string columns to normalize\n",
    "string_columns = [field.name for field in raw_df.schema.fields if field.dataType.simpleString() == \"string\"]\n",
    "\n",
    "# Define UDF to remove accents\n",
    "def remove_accents(text):\n",
    "    if text:\n",
    "        return unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "remove_accents_udf = udf(remove_accents, StringType())\n",
    "\n",
    "# Apply normalization\n",
    "for column in string_columns:\n",
    "    raw_df = raw_df.withColumn(column, remove_accents_udf(col(column)))\n",
    "\n",
    "raw_df.show(5)  # Show first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e09b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+------+----------+--------------+--------------+----+---+---+--------+\n",
      "|producto|     marca|                tipo|precio|     fecha|        estado|        ciudad|anio|mes|dia|catalogo|\n",
      "+--------+----------+--------------------+------+----------+--------------+--------------+----+---+---+--------+\n",
      "|   arroz|precissimo|arroz y cereales ...|  14.2|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun|   dolores|pescados y marisc...|  23.1|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun| el dorado|pescados y marisc...|  13.9|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun| el dorado|pescados y marisc...|  13.9|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun|    herdez|pescados y marisc...|  22.9|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun|    herdez|pescados y marisc...|  23.5|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun|      tuny|pescados y marisc...|  19.9|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|    atun|      tuny|pescados y marisc...|  19.9|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|  azucar|     zulka|              azucar| 104.0|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "|  azucar|     zulka|              azucar|  50.9|2024-01-02|aguascalientes|aguascalientes|2024|  1|  2| basicos|\n",
      "+--------+----------+--------------------+------+----------+--------------+--------------+----+---+---+--------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "#Subimos los datos a S3 en formato parquet\n",
    "raw_df.repartition(\"catalogo\") \\\n",
    "  .write.mode(\"overwrite\") \\\n",
    "  .partitionBy(\"catalogo\") \\\n",
    "  .option(\"compression\", \"snappy\") \\\n",
    "  .parquet(f\"{BUCKET}/{FOLDER}/parquet\")\n",
    "# Comprobamos que se haya subido correctamente\n",
    "df = spark.read.parquet(f\"{BUCKET}/{FOLDER}/parquet\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bd198",
   "metadata": {},
   "source": [
    "## Parte A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902acfba",
   "metadata": {},
   "source": [
    "Contesta las siguientes preguntas utilizando PySpark. Realiza el siguiente análisis (por año) y sobre todos los catálogos.\n",
    "\n",
    "### ¿Cuántos catálogos diferentes tenemos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e7dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de cat?logos diferentes: 10"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Contar los catálogos distintos en el DataFrame\n",
    "num_catalogos = raw_df.select(col(\"catalogo\")).distinct().count()\n",
    "\n",
    "print(f\"Total de catálogos diferentes: {num_catalogos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f949a0f",
   "metadata": {},
   "source": [
    "### ¿Cuáles son los 20 catálogos con más observaciones? Guarda la salida de este query en tu bucket de S3, lo necesitaremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907494cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|           catalogo|  count|\n",
      "+-------------------+-------+\n",
      "|            basicos|1986577|\n",
      "|       medicamentos| 798628|\n",
      "|  electrodomesticos| 267830|\n",
      "| frutas y legumbres| 211838|\n",
      "|              pacic| 155798|\n",
      "|           mercados|  76886|\n",
      "|   utiles escolares|  62102|\n",
      "|pescados y mariscos|  45966|\n",
      "|           juguetes|  19114|\n",
      "|          navidenos|    202|\n",
      "+-------------------+-------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Contar observaciones por catálogo y ordenar en orden descendente\n",
    "top_catalogos = raw_df.groupBy(\"catalogo\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(20)\n",
    "\n",
    "# Mostrar los resultados\n",
    "top_catalogos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82bcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o562.parquet.\n",
      ": org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://s3:/itam-analytics-thmrudolf/profeco/top_catalogos: software.amazon.awssdk.services.s3.model.S3Exception: Bad Request (Service: S3, Status Code: 400, Request ID: VX45NK25BK03AQ0X, Extended Request ID: Jb43GtvmyIpSeVWVIWfEP0h3WgvaaiMGVq/ep4URDBgR3b3vz7ZXEfdDmGssSzq+b8HMvl0ZfMk=):null: Bad Request (Service: S3, Status Code: 400, Request ID: VX45NK25BK03AQ0X, Extended Request ID: Jb43GtvmyIpSeVWVIWfEP0h3WgvaaiMGVq/ep4URDBgR3b3vz7ZXEfdDmGssSzq+b8HMvl0ZfMk=)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:269)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:162)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4317)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4230)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$33(S3AFileSystem.java:5226)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:3108)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:3127)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:5224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:125)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:558)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:597)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:558)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:496)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:99)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:884)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:405)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:365)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:244)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:816)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: software.amazon.awssdk.services.s3.model.S3Exception: Bad Request (Service: S3, Status Code: 400, Request ID: VX45NK25BK03AQ0X, Extended Request ID: Jb43GtvmyIpSeVWVIWfEP0h3WgvaaiMGVq/ep4URDBgR3b3vz7ZXEfdDmGssSzq+b8HMvl0ZfMk=)\n",
      "\tat software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:104)\n",
      "\tat software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:58)\n",
      "\tat software.amazon.awssdk.services.s3.internal.handlers.ExceptionTranslationInterceptor.modifyException(ExceptionTranslationInterceptor.java:88)\n",
      "\tat software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.modifyException(ExecutionInterceptorChain.java:181)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.runModifyException(ExceptionReportingUtils.java:54)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.reportFailureToInterceptors(ExceptionReportingUtils.java:38)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:39)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n",
      "\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\n",
      "\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\n",
      "\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\n",
      "\tat software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:7029)\n",
      "\tat software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$headObject$56(DelegatingS3Client.java:5678)\n",
      "\tat software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\n",
      "\tat software.amazon.awssdk.services.s3.DelegatingS3Client.headObject(DelegatingS3Client.java:5678)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.lambda$headObject$3(S3AStoreImpl.java:602)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.headObject(S3AStoreImpl.java:589)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:3279)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:3259)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4302)\n",
      "\t... 59 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1746280553829_0015/container_1746280553829_0015_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1721, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1746280553829_0015/container_1746280553829_0015_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1746280553829_0015/container_1746280553829_0015_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1746280553829_0015/container_1746280553829_0015_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o562.parquet.\n",
      ": org.apache.hadoop.fs.s3a.AWSBadRequestException: getFileStatus on s3a://s3:/itam-analytics-thmrudolf/profeco/top_catalogos: software.amazon.awssdk.services.s3.model.S3Exception: Bad Request (Service: S3, Status Code: 400, Request ID: VX45NK25BK03AQ0X, Extended Request ID: Jb43GtvmyIpSeVWVIWfEP0h3WgvaaiMGVq/ep4URDBgR3b3vz7ZXEfdDmGssSzq+b8HMvl0ZfMk=):null: Bad Request (Service: S3, Status Code: 400, Request ID: VX45NK25BK03AQ0X, Extended Request ID: Jb43GtvmyIpSeVWVIWfEP0h3WgvaaiMGVq/ep4URDBgR3b3vz7ZXEfdDmGssSzq+b8HMvl0ZfMk=)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:269)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:162)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4317)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4230)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$33(S3AFileSystem.java:5226)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:3108)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:3127)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:5224)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:125)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:558)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:597)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:558)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:126)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:157)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$10(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:384)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:220)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:405)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:219)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:901)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:520)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:303)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:34)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:496)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:99)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:164)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:884)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:405)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:365)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:244)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:816)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: software.amazon.awssdk.services.s3.model.S3Exception: Bad Request (Service: S3, Status Code: 400, Request ID: VX45NK25BK03AQ0X, Extended Request ID: Jb43GtvmyIpSeVWVIWfEP0h3WgvaaiMGVq/ep4URDBgR3b3vz7ZXEfdDmGssSzq+b8HMvl0ZfMk=)\n",
      "\tat software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:104)\n",
      "\tat software.amazon.awssdk.services.s3.model.S3Exception$BuilderImpl.build(S3Exception.java:58)\n",
      "\tat software.amazon.awssdk.services.s3.internal.handlers.ExceptionTranslationInterceptor.modifyException(ExceptionTranslationInterceptor.java:88)\n",
      "\tat software.amazon.awssdk.core.interceptor.ExecutionInterceptorChain.modifyException(ExecutionInterceptorChain.java:181)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.runModifyException(ExceptionReportingUtils.java:54)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.utils.ExceptionReportingUtils.reportFailureToInterceptors(ExceptionReportingUtils.java:38)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:39)\n",
      "\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\n",
      "\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:210)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:173)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:80)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:182)\n",
      "\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:74)\n",
      "\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\n",
      "\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:53)\n",
      "\tat software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:7029)\n",
      "\tat software.amazon.awssdk.services.s3.DelegatingS3Client.lambda$headObject$56(DelegatingS3Client.java:5678)\n",
      "\tat software.amazon.awssdk.services.s3.internal.crossregion.S3CrossRegionSyncClient.invokeOperation(S3CrossRegionSyncClient.java:67)\n",
      "\tat software.amazon.awssdk.services.s3.DelegatingS3Client.headObject(DelegatingS3Client.java:5678)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.lambda$headObject$3(S3AStoreImpl.java:602)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:468)\n",
      "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:431)\n",
      "\tat org.apache.hadoop.fs.s3a.impl.S3AStoreImpl.headObject(S3AStoreImpl.java:589)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:3279)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:3259)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4302)\n",
      "\t... 59 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_catalogos.write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(f\"s3a://{BUCKET}/{FOLDER}/top_catalogos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caade01",
   "metadata": {},
   "source": [
    "### ¿Tenemos datos de todos los estados del país? De no ser así, ¿cuáles faltan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46862d1d",
   "metadata": {},
   "source": [
    "1. Obtener los estados presentes en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e225630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              estado|\n",
      "+--------------------+\n",
      "|          nuevo leon|\n",
      "|            campeche|\n",
      "|             morelos|\n",
      "|          guanajuato|\n",
      "|              oaxaca|\n",
      "|            guerrero|\n",
      "|           queretaro|\n",
      "|    estado de mexico|\n",
      "|              puebla|\n",
      "|             durango|\n",
      "|             jalisco|\n",
      "|      aguascalientes|\n",
      "|coahuila de zaragoza|\n",
      "| baja california sur|\n",
      "|           chihuahua|\n",
      "|     baja california|\n",
      "|    ciudad de mexico|\n",
      "|             hidalgo|\n",
      "| michoacan de ocampo|\n",
      "|             chiapas|\n",
      "+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Obtener lista de estados únicos en los datos\n",
    "estados_en_datos = raw_df.select(col(\"estado\")).distinct()\n",
    "estados_en_datos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec739a91",
   "metadata": {},
   "source": [
    "2. Comparar con la lista completa de estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df26084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estados faltantes en los datos: ['michoacan', 'coahuila', 'colima', 'nayarit']"
     ]
    }
   ],
   "source": [
    "estados_mexico = [\n",
    "    \"aguascalientes\", \"baja california\", \"baja california sur\", \"campeche\", \"coahuila\",\n",
    "    \"colima\", \"chiapas\", \"chihuahua\", \"durango\", \"estado de mexico\", \"guanajuato\", \"guerrero\", \"hidalgo\",\n",
    "    \"jalisco\", \"ciudad de mexico\", \"michoacan\", \"morelos\", \"nayarit\", \"nuevo leon\", \"oaxaca\",\n",
    "    \"puebla\", \"queretaro\", \"quintana roo\", \"san luis potosi\", \"sinaloa\", \"sonora\",\n",
    "    \"tabasco\", \"tamaulipas\", \"tlaxcala\", \"veracruz\", \"yucatan\", \"zacatecas\"\n",
    "]\n",
    "\n",
    "# Convertir PySpark DataFrame a lista para comparación\n",
    "estados_en_datos_lista = [row.estado for row in estados_en_datos.collect()]\n",
    "\n",
    "# Encontrar estados faltantes\n",
    "estados_faltantes = list(set(estados_mexico) - set(estados_en_datos_lista))\n",
    "\n",
    "print(f\"Estados faltantes en los datos: {estados_faltantes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c1db5",
   "metadata": {},
   "source": [
    "### De cada estado obten: el número de catalogos diferentes por año, ¿ha aumentado el número de catálogos con el tiempo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e472334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+\n",
      "|              estado|anio|num_catalogos|\n",
      "+--------------------+----+-------------+\n",
      "|            guerrero|2024|            9|\n",
      "|coahuila de zaragoza|2024|           10|\n",
      "|            tlaxcala|2024|            9|\n",
      "|          nuevo leon|2024|           10|\n",
      "|           queretaro|2024|           10|\n",
      "|             chiapas|2024|            8|\n",
      "|              sonora|2024|            9|\n",
      "|            veracruz|2024|           10|\n",
      "| baja california sur|2024|           10|\n",
      "|          guanajuato|2024|            9|\n",
      "+--------------------+----+-------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Contar catálogos distintos por estado y año\n",
    "catalogos_por_estado_anio = raw_df.groupBy(\"estado\", \"anio\") \\\n",
    "    .agg(countDistinct(\"catalogo\").alias(\"num_catalogos\"))\n",
    "\n",
    "# Mostrar resultados\n",
    "catalogos_por_estado_anio.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbac6a9",
   "metadata": {},
   "source": [
    "Análisis: ¿Ha aumentado el número de catálogos con el tiempo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40310ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+-------------+----------------+\n",
      "|             estado|anio|num_catalogos|diferencia_anual|\n",
      "+-------------------+----+-------------+----------------+\n",
      "|             mz. 54|2024|            1|            NULL|\n",
      "|     aguascalientes|2024|            9|            NULL|\n",
      "|    baja california|2024|           10|            NULL|\n",
      "|baja california sur|2024|           10|            NULL|\n",
      "|           campeche|2024|            9|            NULL|\n",
      "+-------------------+----+-------------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Definir ventana por estado para comparar años anteriores\n",
    "window_spec = Window.partitionBy(\"estado\").orderBy(\"anio\")\n",
    "\n",
    "# Calcular la diferencia de catálogos con el año anterior\n",
    "catalogos_por_estado_anio = catalogos_por_estado_anio.withColumn(\n",
    "    \"diferencia_anual\",\n",
    "    col(\"num_catalogos\") - lag(\"num_catalogos\", 1).over(window_spec)\n",
    ")\n",
    "\n",
    "catalogos_por_estado_anio.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c51c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----+--------+------+-----+------+------+----+---+---+\n",
      "|producto|marca|tipo|catalogo|precio|fecha|estado|ciudad|anio|mes|dia|\n",
      "+--------+-----+----+--------+------+-----+------+------+----+---+---+\n",
      "+--------+-----+----+--------+------+-----+------+------+----+---+---+"
     ]
    }
   ],
   "source": [
    "df_filtered = raw_df.filter(raw_df[\"estado\"] == \"mz. 54\")\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919a08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "print(df_filtered.collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6a7a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
